import sqlite3
from dotenv import load_dotenv
import sys
sys.modules['pysqlite3'] = sys.modules.pop('sqlite3')
import streamlit as st
import os

from langchain_community.llms import OpenAI
from langchain_community.chat_models import ChatOpenAI
from langchain_community.document_loaders import PyPDFLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.embeddings.cohere import CohereEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores.elastic_vector_search import ElasticVectorSearch
from langchain_community.vectorstores import Chroma
from langchain.chat_models import ChatOpenAI
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQAWithSourcesChain





st.set_page_config(
    page_title="í–‰ë³µí•œ ì½”ë”©ë´‡",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)


# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["OPENAI_API_KEY"] = os.getenv("OPEN_API_KEY")

loader = PyPDFLoader('train2.pdf')
documents = loader.load()

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)
embeddings = OpenAIEmbeddings()
vector_store = Chroma.from_documents(texts, embeddings)
retriever = vector_store.as_retriever(search_kwargs={"k": 2})
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)

system_template="""ë‹¹ì‹ ì€ ì¤‘ë“± ì •ë³´(ì»´í“¨í„°) ê³¼ëª© ì„ ìƒë‹˜ì…ë‹ˆë‹¤.
ì‚¬ìš©ìëŠ” íŒŒì´ì¬ì„ ê¸°ì´ˆë¶€í„° í•™ìŠµí•˜ëŠ” í•™ìŠµìì…ë‹ˆë‹¤.
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹¨ê³„ì  í•´ê²° ë°©ë²•ì„ ì œì‹œí•´ì•¼ í•©ë‹ˆë‹¤.
ë‹µë³€ì— ì½”ë“œë¥¼ ì ˆëŒ€ í¬í•¨í•´ì„œëŠ” ì•ˆë©ë‹ˆë‹¤.
íŒŒì´ì¬ê´€ë ¨ ì§ˆë¬¸ì´ ë“¤ì–´ì˜¤ë©´ ì ˆëŒ€ ì½”ë“œë¥¼ ë‹µë³€í•˜ì§€ ì•Šê³ ,
ì½”ë“œë¥¼ ì‘ì„±í•˜ëŠ” ì‚¬ê³ ì™€ ë…¼ë¦¬ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.
----------------
{summaries}

You MUST answer in Korean and in Markdown format:"""

messages = [
    SystemMessagePromptTemplate.from_template(system_template),
    HumanMessagePromptTemplate.from_template("{question}")
]

prompt = ChatPromptTemplate.from_messages(messages)


chain_type_kwargs = {"prompt": prompt}

llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)  # Modify model_name if you have access to GPT-4

chain = RetrievalQAWithSourcesChain.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever = retriever,
    return_source_documents=True,
    chain_type_kwargs=chain_type_kwargs
)


st.subheader('ë„ˆì˜ ì§ˆë¬¸ì„ ì ì–´ì¤˜!')


def generate_response(input_text):
  result = chain(input_text)
  return result['answer']

if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "ì•ˆë…• ë‚˜ëŠ” ì½”ë”© ì±—ë´‡ì´ì•¼! ë¬´ì—‡ì„ ë„ì™€ì¤„ê¹Œ?"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)
    msg =  generate_response(prompt)
    st.session_state.messages.append({"role": "assistant", "content": msg})
    st.chat_message("assistant").write(msg)


